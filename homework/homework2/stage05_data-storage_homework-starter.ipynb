{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 05: Data Storage\n",
    "Name: \n",
    "Date: \n",
    "\n",
    "Objectives:\n",
    "- Env-driven paths to `data/raw/` and `data/processed/`\n",
    "- Save CSV and Parquet; reload and validate\n",
    "- Abstract IO with utility functions; document choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW -> /Users/wenshan/Downloads/data/raw\n",
      "PROC -> /Users/wenshan/Downloads/data/processed\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, datetime as dt\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "RAW = pathlib.Path(os.getenv('DATA_DIR_RAW', 'data/raw'))\n",
    "PROC = pathlib.Path(os.getenv('DATA_DIR_PROCESSED', 'data/processed'))\n",
    "RAW.mkdir(parents=True, exist_ok=True)\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "print('RAW ->', RAW.resolve())\n",
    "print('PROC ->', PROC.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48bf2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env from: /Users/wenshan/Downloads/.env\n",
      "DATA_DIR_RAW: data/raw\n",
      "DATA_DIR_PROCESSED: data/processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "dotenv_path = find_dotenv(usecwd=True)\n",
    "if not dotenv_path:\n",
    "    Path(\".env\").write_text(\"DATA_DIR_RAW=data/raw\\nDATA_DIR_PROCESSED=data/processed\\n\", encoding=\"utf-8\")\n",
    "    dotenv_path = \".env\"\n",
    "\n",
    "load_dotenv(dotenv_path=dotenv_path, override=True)\n",
    "\n",
    "os.environ.setdefault(\"DATA_DIR_RAW\", \"data/raw\")\n",
    "os.environ.setdefault(\"DATA_DIR_PROCESSED\", \"data/processed\")\n",
    "\n",
    "print(\"Loaded .env from:\", Path(dotenv_path).resolve())\n",
    "print(\"DATA_DIR_RAW:\", os.getenv(\"DATA_DIR_RAW\"))\n",
    "print(\"DATA_DIR_PROCESSED:\", os.getenv(\"DATA_DIR_PROCESSED\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Create or Load a Sample DataFrame\n",
    "You may reuse data from prior stages or create a small synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cda09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW dir: /Users/wenshan/Downloads/data/raw\n",
      "PROCESSED dir: /Users/wenshan/Downloads/data/processed\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR_RAW = Path(os.getenv(\"DATA_DIR_RAW\"))\n",
    "DATA_DIR_PROCESSED = Path(os.getenv(\"DATA_DIR_PROCESSED\"))\n",
    "\n",
    "DATA_DIR_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"RAW dir:\", DATA_DIR_RAW.resolve())\n",
    "print(\"PROCESSED dir:\", DATA_DIR_PROCESSED.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>152.719671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>152.669191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>152.625522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>152.681287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>153.168750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date ticker       price\n",
       "0 2024-01-01   AAPL  152.719671\n",
       "1 2024-01-02   AAPL  152.669191\n",
       "2 2024-01-03   AAPL  152.625522\n",
       "3 2024-01-04   AAPL  152.681287\n",
       "4 2024-01-05   AAPL  153.168750"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "dates = pd.date_range('2024-01-01', periods=20, freq='D')\n",
    "df = pd.DataFrame({'date': dates, 'ticker': ['AAPL']*20, 'price': 150 + np.random.randn(20).cumsum()})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Save CSV to data/raw/ and Parquet to data/processed/ (TODO)\n",
    "- Use timestamped filenames.\n",
    "- Handle missing Parquet engine gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153df9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: api_source-alpha_symbol-AAPL_20250818-011602.csv\n",
      "DF_TO_SAVE shape: (63, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>adj_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-16</td>\n",
       "      <td>211.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-19</td>\n",
       "      <td>208.543320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>206.625504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>201.860901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>201.131729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   adj_close\n",
       "0 2025-05-16  211.020508\n",
       "1 2025-05-19  208.543320\n",
       "2 2025-05-20  206.625504\n",
       "3 2025-05-21  201.860901\n",
       "4 2025-05-22  201.131729"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "RAW_DIR = Path(os.getenv(\"DATA_DIR_RAW\", \"data/raw\"))\n",
    "\n",
    "preferred_prefix = \"api_\"  \n",
    "candidates = sorted(RAW_DIR.glob(preferred_prefix + \"*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not candidates:\n",
    "    fallback = \"web_\" if preferred_prefix == \"api_\" else \"api_\"\n",
    "    candidates = sorted(RAW_DIR.glob(fallback + \"*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(f\"No raw CSVs found under {RAW_DIR}. Expected files like api_*.csv or web_*.csv\")\n",
    "\n",
    "csv_path = candidates[0] \n",
    "print(\"Loading:\", csv_path.name)\n",
    "DF_TO_SAVE = pd.read_csv(csv_path)\n",
    "\n",
    "for col in [\"date\"]:\n",
    "    if col in DF_TO_SAVE.columns:\n",
    "        DF_TO_SAVE[col] = pd.to_datetime(DF_TO_SAVE[col], errors=\"coerce\")\n",
    "\n",
    "print(\"DF_TO_SAVE shape:\", DF_TO_SAVE.shape)\n",
    "DF_TO_SAVE.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet engine not available. Install pyarrow or fastparquet to complete this step.\n"
     ]
    }
   ],
   "source": [
    "def ts(): return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# TODO: Save CSV\n",
    "csv_path = RAW / f\"sample_{ts()}.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "csv_path\n",
    "\n",
    "# TODO: Save Parquet\n",
    "pq_path = PROC / f\"sample_{ts()}.parquet\"\n",
    "try:\n",
    "    df.to_parquet(pq_path)\n",
    "except Exception as e:\n",
    "    print('Parquet engine not available. Install pyarrow or fastparquet to complete this step.')\n",
    "    pq_path = None\n",
    "pq_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Reload and Validate (TODO)\n",
    "- Compare shapes and key dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shape_equal': False, 'date_is_datetime': True, 'price_is_numeric': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_loaded(original, reloaded):\n",
    "    checks = {\n",
    "        'shape_equal': original.shape == reloaded.shape,\n",
    "        'date_is_datetime': pd.api.types.is_datetime64_any_dtype(reloaded['date']) if 'date' in reloaded.columns else False,\n",
    "        'price_is_numeric': pd.api.types.is_numeric_dtype(reloaded['price']) if 'price' in reloaded.columns else False,\n",
    "    }\n",
    "    return checks\n",
    "\n",
    "df_csv = pd.read_csv(csv_path, parse_dates=['date'])\n",
    "validate_loaded(df, df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pq_path:\n",
    "    try:\n",
    "        df_pq = pd.read_parquet(pq_path)\n",
    "        validate_loaded(df, df_pq)\n",
    "    except Exception as e:\n",
    "        print('Parquet read failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Utilities (TODO)\n",
    "- Implement `detect_format`, `write_df`, `read_df`.\n",
    "- Use suffix to route; create parent dirs if needed; friendly errors for Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b721d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR_RAW = Path(os.getenv(\"DATA_DIR_RAW\", \"data/raw\"))\n",
    "DATA_DIR_PROCESSED = Path(os.getenv(\"DATA_DIR_PROCESSED\", \"data/processed\"))\n",
    "DATA_DIR_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _ts() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4096cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _choose_parquet_engine() -> str:\n",
    "    \"\"\"\n",
    "    Prefer pyarrow; fall back to fastparquet. Raise with helpful msg if neither.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        return \"pyarrow\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            import fastparquet  # noqa: F401\n",
    "            return \"fastparquet\"\n",
    "        except Exception:\n",
    "            raise RuntimeError(\n",
    "                \"No Parquet engine found. Install one:\\n\"\n",
    "                \"  pip install pyarrow   # recommended\\n\"\n",
    "                \"  # or\\n\"\n",
    "                \"  pip install fastparquet\"\n",
    "            )\n",
    "\n",
    "def _build_filename(prefix: str, fmt: str, **meta) -> str:\n",
    "    \"\"\"\n",
    "    Make a filename like: {prefix}_{k1-v1}_{k2-v2}_{timestamp}.{fmt}\n",
    "    Only include meta with non-empty values; keys sorted for stability.\n",
    "    \"\"\"\n",
    "    parts = [prefix]\n",
    "    for k in sorted(meta.keys()):\n",
    "        v = str(meta[k]).strip()\n",
    "        if v != \"\":\n",
    "            parts.append(f\"{k}-{v}\")\n",
    "    parts.append(_ts())\n",
    "    return \"_\".join(parts) + f\".{fmt}\"\n",
    "\n",
    "def write_df(\n",
    "    df: pd.DataFrame,\n",
    "    area: str = \"raw\",    # 'raw' or 'processed' (selects env dir)\n",
    "    prefix: str = \"sample\",\n",
    "    fmt: str = \"csv\",     # 'csv' or 'parquet'\n",
    "    index: bool = False,\n",
    "    **meta,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Write a DataFrame to CSV or Parquet under the env-driven directory.\n",
    "    Returns the absolute Path written. Adds metadata to filename.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"write_df: df must be a pandas DataFrame\")\n",
    "    if fmt not in {\"csv\", \"parquet\"}:\n",
    "        raise ValueError(\"write_df: fmt must be 'csv' or 'parquet'\")\n",
    "    if area not in {\"raw\", \"processed\"}:\n",
    "        raise ValueError(\"write_df: area must be 'raw' or 'processed'\")\n",
    "\n",
    "    base_dir = DATA_DIR_RAW if area == \"raw\" else DATA_DIR_PROCESSED\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fname = _build_filename(prefix=prefix, fmt=fmt, **meta)\n",
    "    out_path = (base_dir / fname).resolve()\n",
    "\n",
    "    if fmt == \"csv\":\n",
    "        df.to_csv(out_path, index=index)\n",
    "    else:\n",
    "        engine = _choose_parquet_engine()\n",
    "        df.to_parquet(out_path, index=index, engine=engine)\n",
    "\n",
    "    print(f\"Saved ({fmt}) → {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "def read_df(path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a DataFrame back from CSV/Parquet by file extension.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"read_df: path not found: {p}\")\n",
    "    ext = p.suffix.lower()\n",
    "    if ext == \".csv\":\n",
    "        return pd.read_csv(p)\n",
    "    elif ext == \".parquet\":\n",
    "        # Try both engines transparently\n",
    "        try:\n",
    "            return pd.read_parquet(p, engine=\"pyarrow\")\n",
    "        except Exception:\n",
    "            return pd.read_parquet(p, engine=\"fastparquet\")\n",
    "    else:\n",
    "        raise ValueError(f\"read_df: unsupported extension '{ext}' (use .csv or .parquet)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54507ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DataFrame in memory: DF_TO_SAVE shape: (63, 2)\n",
      "Saved (csv) → /Users/wenshan/Downloads/data/raw/utiltest_note-csv_source-utilities_20250818-013433.csv\n",
      "Parquet engine missing → No Parquet engine found. Install one:\n",
      "  pip install pyarrow   # recommended\n",
      "  # or\n",
      "  pip install fastparquet\n",
      "Read-back CSV shape: (63, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DF_CHECK = None\n",
    "for cand in [\"DF_TO_SAVE\", \"df_api\", \"df_web\"]:\n",
    "    if cand in globals():\n",
    "        DF_CHECK = globals()[cand]\n",
    "        which = cand\n",
    "        break\n",
    "\n",
    "if DF_CHECK is None:\n",
    "    raise RuntimeError(\"No DataFrame found in memory. Load one CSV first (your Stage 05 prep cell).\")\n",
    "\n",
    "print(\"Using DataFrame in memory:\", which, \"shape:\", DF_CHECK.shape)\n",
    "\n",
    "csv_path = write_df(\n",
    "    DF_CHECK,\n",
    "    area=\"raw\",\n",
    "    prefix=\"utiltest\",\n",
    "    fmt=\"csv\",\n",
    "    source=\"utilities\",\n",
    "    note=\"csv\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    pq_path = write_df(\n",
    "        DF_CHECK,\n",
    "        area=\"processed\",\n",
    "        prefix=\"utiltest\",\n",
    "        fmt=\"parquet\",\n",
    "        source=\"utilities\",\n",
    "        note=\"parquet\"\n",
    "    )\n",
    "    parquet_ok = True\n",
    "except RuntimeError as e:\n",
    "    print(\"Parquet engine missing →\", e)\n",
    "    parquet_ok = False\n",
    "    pq_path = None\n",
    "\n",
    "df_csv_back = read_df(csv_path)\n",
    "print(\"Read-back CSV shape:\", df_csv_back.shape)\n",
    "\n",
    "if parquet_ok:\n",
    "    df_pq_back = read_df(pq_path)\n",
    "    print(\"Read-back Parquet shape:\", df_pq_back.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "762f6218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /opt/miniconda3/bin/python\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-21.0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "print(\"Python:\", sys.executable)\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"pyarrow\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49c6438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /opt/miniconda3/bin/python\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2024.11.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/miniconda3/lib/python3.13/site-packages (from fastparquet) (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.13/site-packages (from fastparquet) (2.3.2)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Downloading cramjam-2.11.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting fsspec (from fastparquet)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.13/site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.13/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.13/site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.13/site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "Downloading fastparquet-2024.11.0-cp313-cp313-macosx_11_0_arm64.whl (683 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.8/683.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cramjam-2.11.0-cp313-cp313-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Installing collected packages: fsspec, cramjam, fastparquet\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [fastparquet]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cramjam-2.11.0 fastparquet-2024.11.0 fsspec-2025.7.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "print(\"Python:\", sys.executable)\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"fastparquet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df80300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV round-trip: (20, 3)\n",
      "Parquet round-trip: (20, 3)\n"
     ]
    }
   ],
   "source": [
    "import typing as t, pathlib, pandas as pd, numpy as np\n",
    "\n",
    "def _choose_parquet_engine() -> str:\n",
    "    \"\"\"Prefer pyarrow; fall back to fastparquet; raise helpful message if neither.\"\"\"\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        return \"pyarrow\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            import fastparquet  # noqa: F401\n",
    "            return \"fastparquet\"\n",
    "        except Exception:\n",
    "            raise RuntimeError(\n",
    "                \"Parquet engine not available. Install one of:\\n\"\n",
    "                \"  pip install pyarrow   # recommended\\n\"\n",
    "                \"  # or\\n\"\n",
    "                \"  pip install fastparquet\"\n",
    "            )\n",
    "\n",
    "def _parquet_sanitize_for_write(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make df easy for parquet writers:\n",
    "      - plain string column names\n",
    "      - tz-naive datetimes\n",
    "      - convert list/dict objects to JSON strings\n",
    "      - cast mixed-type object columns to string\n",
    "    \"\"\"\n",
    "    x = df.copy()\n",
    "\n",
    "    # 1) column names to strings\n",
    "    x.columns = [str(c) for c in x.columns]\n",
    "\n",
    "    # 2) tz-naive datetimes\n",
    "    for c in x.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(x[c]):\n",
    "            try:\n",
    "                # If tz-aware, normalize to UTC then drop tz\n",
    "                if getattr(x[c].dt, \"tz\", None) is not None:\n",
    "                    x[c] = x[c].dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "            except Exception:\n",
    "                # If parsing fails, coerce and drop tz anyway\n",
    "                x[c] = pd.to_datetime(x[c], errors=\"coerce\")\n",
    "            # ensure dtype is datetime64[ns]\n",
    "            x[c] = pd.to_datetime(x[c], errors=\"coerce\")\n",
    "\n",
    "    # 3) object columns: lists/dicts -> JSON, mixed -> str\n",
    "    import json\n",
    "    for c in x.columns:\n",
    "        if x[c].dtype == \"object\":\n",
    "            sample = x[c].dropna().head(10).tolist()\n",
    "            if any(isinstance(v, (list, dict)) for v in sample):\n",
    "                x[c] = x[c].apply(lambda v: json.dumps(v) if isinstance(v, (list, dict)) else v)\n",
    "            # detect mixed types (cheap heuristic)\n",
    "            types = {type(v) for v in x[c].dropna().head(25)}\n",
    "            if len(types) > 1:\n",
    "                x[c] = x[c].astype(str)\n",
    "\n",
    "    return x\n",
    "\n",
    "def _csv_detect_date_cols(header_only_df: pd.DataFrame) -> t.List[str]:\n",
    "    \"\"\"\n",
    "    Given a header-only DataFrame (nrows=0), decide which columns to parse as dates.\n",
    "    If a 'date' column exists, we parse just that to avoid surprises.\n",
    "    \"\"\"\n",
    "    cols = list(header_only_df.columns)\n",
    "    return ['date'] if 'date' in cols else []\n",
    "\n",
    "def detect_format(path: t.Union[str, pathlib.Path]):\n",
    "    s = str(path).lower()\n",
    "    if s.endswith('.csv'): return 'csv'\n",
    "    if s.endswith('.parquet') or s.endswith('.pq') or s.endswith('.parq'): return 'parquet'\n",
    "    raise ValueError('Unsupported format: ' + s)\n",
    "\n",
    "def write_df(df: pd.DataFrame, path: t.Union[str, pathlib.Path]):\n",
    "    \"\"\"\n",
    "    Write CSV or Parquet. For Parquet:\n",
    "      - auto-picks engine (pyarrow -> fastparquet)\n",
    "      - retries with sanitized copy if the first attempt fails\n",
    "      - raises a clear message if engines are missing\n",
    "    \"\"\"\n",
    "    p = pathlib.Path(path); p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fmt = detect_format(p)\n",
    "\n",
    "    if fmt == 'csv':\n",
    "        df.to_csv(p, index=False)\n",
    "        return p\n",
    "\n",
    "    # parquet branch\n",
    "    engine = _choose_parquet_engine()\n",
    "    try:\n",
    "        df.to_parquet(p, index=False, engine=engine)\n",
    "        return p\n",
    "    except Exception as e:\n",
    "        # retry once with sanitized data\n",
    "        try:\n",
    "            clean = _parquet_sanitize_for_write(df)\n",
    "            clean.to_parquet(p, index=False, engine=engine)\n",
    "            return p\n",
    "        except Exception as e2:\n",
    "            # last resort: if we have pyarrow, try lower-level conversion for a better error\n",
    "            try:\n",
    "                if engine == \"pyarrow\":\n",
    "                    import pyarrow as pa, pyarrow.parquet as pq\n",
    "                    table = pa.Table.from_pandas(_parquet_sanitize_for_write(df), preserve_index=False, safe=False)\n",
    "                    pq.write_table(table, p)\n",
    "                    return p\n",
    "            except Exception as e3:\n",
    "                pass\n",
    "            raise RuntimeError(\n",
    "                f\"Parquet write failed with engine '{engine}'. \"\n",
    "                \"Tried with sanitized data too. Common fixes:\\n\"\n",
    "                \"  * Ensure all column names are unique strings\\n\"\n",
    "                \"  * Convert mixed-type object columns to string\\n\"\n",
    "                \"  * Make datetimes tz-naive\\n\"\n",
    "                \"If still failing, try the other engine:\\n\"\n",
    "                \"  pip install fastparquet   # or pyarrow\\n\"\n",
    "                f\"Original error: {repr(e)}\\n\"\n",
    "                f\"Sanitized error: {repr(e2)}\"\n",
    "            ) from e\n",
    "\n",
    "def read_df(path: t.Union[str, pathlib.Path]):\n",
    "    \"\"\"\n",
    "    Read CSV (parse 'date' column if present) or Parquet (auto engine).\n",
    "    \"\"\"\n",
    "    p = pathlib.Path(path)\n",
    "    fmt = detect_format(p)\n",
    "\n",
    "    if fmt == 'csv':\n",
    "        # read header once, decide parse_dates, then load the full CSV\n",
    "        header = pd.read_csv(p, nrows=0)\n",
    "        parse_dates = _csv_detect_date_cols(header)\n",
    "        if parse_dates:\n",
    "            return pd.read_csv(p, parse_dates=parse_dates)\n",
    "        else:\n",
    "            return pd.read_csv(p)\n",
    "\n",
    "    # parquet\n",
    "    try:\n",
    "        return pd.read_parquet(p, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_parquet(p, engine=\"fastparquet\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError('Parquet engine not available or file unreadable with available engines. '\n",
    "                               'Install pyarrow or fastparquet.') from e\n",
    "\n",
    "p_csv = RAW / f\"util_{ts()}.csv\"\n",
    "p_pq  = PROC / f\"util_{ts()}.parquet\"\n",
    "\n",
    "write_df(df, p_csv)\n",
    "print(\"CSV round-trip:\", read_df(p_csv).shape)\n",
    "\n",
    "try:\n",
    "    write_df(df, p_pq)\n",
    "    print(\"Parquet round-trip:\", read_df(p_pq).shape)\n",
    "except RuntimeError as e:\n",
    "    print('Skipping Parquet util demo:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Parquet util demo: Parquet engine not available. Install pyarrow or fastparquet.\n"
     ]
    }
   ],
   "source": [
    "import typing as t, pathlib\n",
    "\n",
    "def detect_format(path: t.Union[str, pathlib.Path]):\n",
    "    s = str(path).lower()\n",
    "    if s.endswith('.csv'): return 'csv'\n",
    "    if s.endswith('.parquet') or s.endswith('.pq') or s.endswith('.parq'): return 'parquet'\n",
    "    raise ValueError('Unsupported format: ' + s)\n",
    "\n",
    "def write_df(df: pd.DataFrame, path: t.Union[str, pathlib.Path]):\n",
    "    p = pathlib.Path(path); p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fmt = detect_format(p)\n",
    "    if fmt == 'csv':\n",
    "        df.to_csv(p, index=False)\n",
    "    else:\n",
    "        try:\n",
    "            df.to_parquet(p)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError('Parquet engine not available. Install pyarrow or fastparquet.') from e\n",
    "    return p\n",
    "\n",
    "def read_df(path: t.Union[str, pathlib.Path]):\n",
    "    p = pathlib.Path(path)\n",
    "    fmt = detect_format(p)\n",
    "    if fmt == 'csv':\n",
    "        return pd.read_csv(p, parse_dates=['date']) if 'date' in pd.read_csv(p, nrows=0).columns else pd.read_csv(p)\n",
    "    else:\n",
    "        try:\n",
    "            return pd.read_parquet(p)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError('Parquet engine not available. Install pyarrow or fastparquet.') from e\n",
    "\n",
    "# Demo\n",
    "p_csv = RAW / f\"util_{ts()}.csv\"\n",
    "p_pq  = PROC / f\"util_{ts()}.parquet\"\n",
    "write_df(df, p_csv); read_df(p_csv).head()\n",
    "try:\n",
    "    write_df(df, p_pq)\n",
    "    read_df(p_pq).head()\n",
    "except RuntimeError as e:\n",
    "    print('Skipping Parquet util demo:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Documentation (TODO)\n",
    "- Update README with a **Data Storage** section (folders, formats, env usage).\n",
    "- Summarize validation checks and any assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b69bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newest RAW CSV: util_20250818-014145.csv\n",
      "Newest PROCESSED Parquet: util_20250818-014145.parquet\n",
      "\n",
      "CSV reload → shape: (20, 3) | Validation: {'missing': [], 'shape': (20, 3), 'na_total': 0}\n",
      "Parquet reload → shape: (20, 3) | Validation: {'missing': [], 'shape': (20, 3), 'na_total': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>152.719671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>152.669191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>152.625522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date ticker       price\n",
       "0 2024-01-01   AAPL  152.719671\n",
       "1 2024-01-02   AAPL  152.669191\n",
       "2 2024-01-03   AAPL  152.625522"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>152.719671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>152.669191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>152.625522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date ticker       price\n",
       "0 2024-01-01   AAPL  152.719671\n",
       "1 2024-01-02   AAPL  152.669191\n",
       "2 2024-01-03   AAPL  152.625522"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "RAW  = Path(os.getenv(\"DATA_DIR_RAW\", \"data/raw\"))\n",
    "PROC = Path(os.getenv(\"DATA_DIR_PROCESSED\", \"data/processed\"))\n",
    "\n",
    "def newest(glob_iter):\n",
    "    files = sorted(glob_iter, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return files[0] if files else None\n",
    "\n",
    "csv_path = newest(RAW.glob(\"*.csv\"))\n",
    "pq_path  = newest(PROC.glob(\"*.parquet\"))\n",
    "\n",
    "print(\"Newest RAW CSV:\", csv_path.name if csv_path else \"None found\")\n",
    "print(\"Newest PROCESSED Parquet:\", pq_path.name if pq_path else \"None found\")\n",
    "\n",
    "if 'read_df' not in globals():\n",
    "    raise RuntimeError(\"read_df() not found — run your Utilities cell first.\")\n",
    "\n",
    "df_csv = read_df(csv_path) if csv_path else None\n",
    "df_pq  = read_df(pq_path) if pq_path else None\n",
    "\n",
    "def _fallback_validate(df: pd.DataFrame, required_cols):\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    out = {\"missing\": missing, \"shape\": df.shape, \"na_total\": int(df.isna().sum().sum())}\n",
    "    return out\n",
    "\n",
    "def _smart_required(df: pd.DataFrame):\n",
    "    cols = set(df.columns.str.lower())\n",
    "    if {\"date\",\"adj_close\"}.issubset(cols):\n",
    "        return [\"date\",\"adj_close\"]\n",
    "    elif {\"symbol\",\"security\"}.intersection(cols):\n",
    "        need = []\n",
    "        for c in [\"symbol\",\"security\"]:\n",
    "            if c in cols: need.append(c)\n",
    "        return need\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def _validate(df: pd.DataFrame):\n",
    "    req = _smart_required(df)\n",
    "    if 'validate' in globals():\n",
    "        try:\n",
    "            return validate(df, req)  # your notebook's helper\n",
    "        except Exception:\n",
    "            return _fallback_validate(df, req)\n",
    "    else:\n",
    "        return _fallback_validate(df, req)\n",
    "\n",
    "if df_csv is not None:\n",
    "    v_csv = _validate(df_csv)\n",
    "    print(\"\\nCSV reload → shape:\", df_csv.shape, \"| Validation:\", v_csv)\n",
    "\n",
    "if df_pq is not None:\n",
    "    v_pq = _validate(df_pq)\n",
    "    print(\"Parquet reload → shape:\", df_pq.shape, \"| Validation:\", v_pq)\n",
    "\n",
    "if df_csv is not None:\n",
    "    display(df_csv.head(3))\n",
    "if df_pq is not None:\n",
    "    display(df_pq.head(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
